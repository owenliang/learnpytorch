{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "基于transformer的翻译模型 - pytorch教程\n",
    "https://pytorch.org/tutorials/beginner/translation_transformer.html\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "'''\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词: {'de': functools.partial(<function _spacy_tokenize at 0x0000015C795C4DC0>, spacy=<spacy.lang.de.German object at 0x0000015C88A18FA0>), 'en': functools.partial(<function _spacy_tokenize at 0x0000015C795C4DC0>, spacy=<spacy.lang.en.English object at 0x0000015C4D064430>)}\n",
      "词表: {'de': Vocab(), 'en': Vocab()}\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "第一部分：词表生成\n",
    "'''\n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "# de和en各自的分词器\n",
    "token_transform = {}\n",
    "# de和en各自的词表\n",
    "vocab_transform = {}\n",
    "\n",
    "# 分词器\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "# 句子分词\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        # 对句子分词\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# 特殊字符（未知,填充,开始,结束)\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "# 从训练数据中, 分别生成en和de的词表\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    # 训练集\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    # 构建词表\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)\n",
    "\n",
    "print('分词:', token_transform)\n",
    "print('词表:', vocab_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " 第二部分：翻译模型的定义\n",
    "'''\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "# 位置编码，会在模型里加到词向量上面\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,         # 每个位置向量宽度\n",
    "                 dropout: float,    # dropout率\n",
    "                 maxlen: int = 5000): # 最多5000个词输入\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size) # 生成emb_size/2宽的向量\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1) # 为每个词生成序号，竖起来\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size)) # 每个位置对应一个位置向量\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den) # 每个位置向量的偶数位置填充pos序号*sin\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den) # 每个位置向量的奇数位置填充pos序号*cos\n",
    "        print('pos shape:', pos.size(), 'pos data:', pos)\n",
    "        print('pos_embedding shape:', pos_embedding.size(), 'pos_embedding data:', pos_embedding)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2) # 每个位置加上batch维\n",
    "        print('pos_embedding unsqueeze -2 shape:', pos_embedding.size())\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding) # 固定向量\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        # 输入token序列长度在0维,所以pos emb只加到token序列的长度为止\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :]) # 输入的token向量序列和对应位置的pos向量相加\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "# 词id序列转emb序列\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size) \n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        # embedding对矩阵中每个词ID替换emb vector，不改变其他形状\n",
    "        # tokens原本是(seq_size,batch_size),处理后是(seq_size,batch_size,emb_size)\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size) # 词id序列分别查emb向量,还elem-wise乘了一下sqrt emb size\n",
    "\n",
    "# Seq2Seq Network\n",
    "# 序列生成模型\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        # transofmer is all you need 模型，encoder+decoder架构\n",
    "        self.transformer = Transformer(d_model=emb_size, # 即encoder/decoder的输入词向量宽度\n",
    "                                       nhead=nhead, # 注意力多头个数\n",
    "                                       num_encoder_layers=num_encoder_layers, # encoder阶段堆叠(网络结构图中可堆叠的部分)\n",
    "                                       num_decoder_layers=num_decoder_layers, # decoder阶段堆叠(网络结构图中可堆叠的部分)\n",
    "                                       dim_feedforward=dim_feedforward, # feedforward结构的神经元个数\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size) # 接在encoder后面预测下一个词, 输入emb pooling, 神经元个数是翻译目标语言的词表大小\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size) # 将输入词id转成emb vector\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size) # 将输出词id转成emb vector\n",
    "        self.positional_encoding = PositionalEncoding( # 位置向量\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src)) # 输入词Id序列->输入词emb序列->给每个位置的词emb叠加pos emb\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg)) # 输出词id序列->输出词emb序列->给每个位置的词emb叠加pos emb\n",
    "        # src_emb和tgt_emb都是(seq_size,batch_size,emb_size)形状\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, # tgt_mask限制outputs输入时每个pos可以看到其他pos的范围\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)  # src_padding_mask和tgt_padding_mask告知每个pos上哪些句子是pad的占位符\n",
    "        # decoder输出emb宽的pooling向量, 再过linear转词概率预测\n",
    "        return self.generator(outs)\n",
    "\n",
    "    # 推理时单独调用Encoder阶段\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    # 推理时单独调用Decoder阶段\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos shape: torch.Size([5000, 1]) pos data: tensor([[   0],\n",
      "        [   1],\n",
      "        [   2],\n",
      "        ...,\n",
      "        [4997],\n",
      "        [4998],\n",
      "        [4999]])\n",
      "pos_embedding shape: torch.Size([5000, 512]) pos_embedding data: tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
      "          1.0366e-04,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
      "          2.0733e-04,  1.0000e+00],\n",
      "        ...,\n",
      "        [ 9.5625e-01, -2.9254e-01,  9.3594e-01,  ...,  8.5926e-01,\n",
      "          4.9515e-01,  8.6881e-01],\n",
      "        [ 2.7050e-01, -9.6272e-01,  8.2251e-01,  ...,  8.5920e-01,\n",
      "          4.9524e-01,  8.6876e-01],\n",
      "        [-6.6395e-01, -7.4778e-01,  1.4615e-03,  ...,  8.5915e-01,\n",
      "          4.9533e-01,  8.6871e-01]])\n",
      "pos_embedding unsqueeze -2 shape: torch.Size([5000, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    " 第三部分：准备训练阶段\n",
    "'''\n",
    "\n",
    "# 掩码是加到attention score上面的，这样-inf加上去就导致softmax为0，起到了忽略输入的效果\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1) # Pos=0的词只注意自己，pos=1的词注意pos=0和pos=1，pos=2的词注意pos=0,pos=1,pos=2\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)) # 标0的转-inf float，其他标0\n",
    "    return mask\n",
    "\n",
    "# 参数是词id序列，返回模型所需的mask(attention mask和padding mask)\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0] # 输入序列的词个数\n",
    "    tgt_seq_len = tgt.shape[0] # 输出序列的词个数\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len) # 输出序列的mask，要mask住每个词后面的部分\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool) # 输入序列的mask，是全0的，不做任何mask\n",
    "\n",
    "    # PAD填充位置填1,其他填0\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1) \n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE]) # de输入词表大小\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE]) # en输出词表大小\n",
    "EMB_SIZE = 512 # 词向量宽度\n",
    "NHEAD = 8   # 自注意力多头个数\n",
    "FFN_HID_DIM = 512   # feedforward保持输出emb宽\n",
    "BATCH_SIZE = 128    \n",
    "NUM_ENCODER_LAYERS = 3 # encoder堆叠三层encoder\n",
    "NUM_DECODER_LAYERS = 3 # decoder堆叠三层decoder\n",
    "\n",
    "# 定义seq2seq模型\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "# 所有模型参数\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1: \n",
    "        nn.init.xavier_uniform_(p)  # 均匀分布初始化参数初始值\n",
    "\n",
    "# 模型放到GPU上\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "# linear预测的下一个词概率和真实下一个词求损失\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "# 组装数据预处理流水线：分词->ID化->添加BOS和EOS id\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "# 准备de和en两种句子的预处理方法, 即构造流水线: 分词->id化->添加[BOS]和[EOS]\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "# 输入1批样本\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    # 对于每一对de句子和en句子\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))) # de的句子id序列\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\"))) # en的句子id序列\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)    # 这一batch的输入句子对齐长度,返回(max_seq_size,batch_size)的形状\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)    # 这一batch的输出句子对齐长度,返回(max_seq_size,batch_size)的形状\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_mask: tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False]], device='cuda:0')\n",
      "Epoch: 1, Train loss: 0.000, Val loss: 0.000, Epoch time = 2.272s\n",
      "src_mask: tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False]], device='cuda:0')\n",
      "Epoch: 2, Train loss: 0.000, Val loss: 0.000, Epoch time = 2.199s\n",
      "src_mask: tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, NUM_EPOCHS\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     48\u001b[0m     start_time \u001b[39m=\u001b[39m timer()\n\u001b[1;32m---> 49\u001b[0m     train_loss \u001b[39m=\u001b[39m train_epoch(transformer, optimizer)\n\u001b[0;32m     50\u001b[0m     end_time \u001b[39m=\u001b[39m timer()\n\u001b[0;32m     51\u001b[0m     \u001b[39m#val_loss = evaluate(transformer)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 44\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, optimizer)\u001b[0m\n\u001b[0;32m     41\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     42\u001b[0m     losses \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> 44\u001b[0m \u001b[39mreturn\u001b[39;00m losses \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39;49m(train_dataloader))\n",
      "File \u001b[1;32mc:\\Users\\owenliang\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\owenliang\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\owenliang\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:42\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_iter)\n\u001b[1;32m---> 42\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[1;32mIn[28], line 83\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39m# 对于每一对de句子和en句子\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39mfor\u001b[39;00m src_sample, tgt_sample \u001b[39min\u001b[39;00m batch:\n\u001b[1;32m---> 83\u001b[0m     src_batch\u001b[39m.\u001b[39mappend(text_transform[SRC_LANGUAGE](src_sample\u001b[39m.\u001b[39;49mrstrip(\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m))) \u001b[39m# de的句子id序列\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     tgt_batch\u001b[39m.\u001b[39mappend(text_transform[TGT_LANGUAGE](tgt_sample\u001b[39m.\u001b[39mrstrip(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m))) \u001b[39m# en的句子id序列\u001b[39;00m\n\u001b[0;32m     86\u001b[0m src_batch \u001b[39m=\u001b[39m pad_sequence(src_batch, padding_value\u001b[39m=\u001b[39mPAD_IDX)    \u001b[39m# 这一batch的输入句子对齐长度,返回(max_seq_size,batch_size)的形状\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 58\u001b[0m, in \u001b[0;36msequential_transforms.<locals>.func\u001b[1;34m(txt_input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(txt_input):\n\u001b[0;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m transform \u001b[39min\u001b[39;00m transforms:\n\u001b[1;32m---> 58\u001b[0m         txt_input \u001b[39m=\u001b[39m transform(txt_input)\n\u001b[0;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m txt_input\n",
      "Cell \u001b[1;32mIn[28], line 64\u001b[0m, in \u001b[0;36mtensor_transform\u001b[1;34m(token_ids)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtensor_transform\u001b[39m(token_ids: List[\u001b[39mint\u001b[39m]):\n\u001b[1;32m---> 64\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mcat((torch\u001b[39m.\u001b[39;49mtensor([BOS_IDX]),\n\u001b[0;32m     65\u001b[0m                       torch\u001b[39m.\u001b[39;49mtensor(token_ids),\n\u001b[0;32m     66\u001b[0m                       torch\u001b[39m.\u001b[39;49mtensor([EOS_IDX])))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' \n",
    " 第四部分：开始训练\n",
    "''' \n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train() # 训练状态(dropout生效)\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE)) # 数据集\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn) # 数据迭代器\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        # 样本放入GPU\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        # src和tgt是(seq_len,batch_size)形状的\n",
    "        #print('src size:', src.size(), 'tgt size:', tgt.size()) # src size: torch.Size([27, 128]) tgt size: torch.Size([24, 128])\n",
    "\n",
    "        # decoder只输入[0,N-1)位置的token,以便让它预测[1,N-1]位置的token\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        #print('tgt_input size', tgt_input.size()) # tgt_input size torch.Size([23, 128])\n",
    "\n",
    "        # tgt_mask解决的是decoder不同pos对其他pos可见性问题，pad mask解决的是batch对齐后无效pos问题\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        #print('src_mask size:', src_mask.size(), 'tgt_mask size:', tgt_mask.size()) # src_mask size: torch.Size([27, 27]) tgt_mask size: torch.Size([23, 23])\n",
    "        #print('src_padding_mask size:', src_padding_mask.size(), 'tgt_padding_mask size:', tgt_padding_mask.size()) # src_padding_mask size: torch.Size([128, 27]) tgt_padding_mask size: torch.Size([128, 23])\n",
    "        # print('src_mask:',src_mask)\n",
    "        # print('tgt_mask:', tgt_mask)\n",
    " \n",
    "        # forward，依据每个样本[0,tgt_size)位置的token，预测出[1,tgt_size]位置的token\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        #print('logits size:', logits.size()) # logits size: torch.Size([23, 128, 10837]), (词序列长度,批大小,词表大小) -> (每个句子23个词,128个句子,每个词有10837种可能)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 计算每个样本[1,tgt_size]这些token id和预测出的[1,tgt_size]位置token概率的误差\n",
    "        tgt_out = tgt[1:, :]\n",
    "        #print('tgt_out size:', tgt_out.size())\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1)) # 把batch维直接去掉,这样就是每个样本的每个token的logis和每个样本的每个token id一一对应求loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "NUM_EPOCHS = 16\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    #val_loss = evaluate(transformer)\n",
    "    val_loss=0\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tranlate src: torch.Size([11, 1])\n",
      "src_mask src: torch.Size([11, 11])\n",
      " A group of people standing in front of an igloo . \n"
     ]
    }
   ],
   "source": [
    "''' \n",
    " 第五部分：推理\n",
    "'''\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0)) # 有几个词就做几个词的attention mask向量\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "# 翻译方法, 输入de句子\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval() # 推理模式(dropout关闭)\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1) # src是(seq_size,batch_size),和train时候的dim顺序一样\n",
    "    print('tranlate src:',src.shape)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    print('src_mask src:',src_mask.shape)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n",
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
