{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_1 tensor([[0.4620, 0.5982, 0.9980],\n",
      "        [0.6983, 0.4307, 0.8695],\n",
      "        [0.8155, 0.7759, 1.4014],\n",
      "        [0.7517, 0.6991, 1.0948],\n",
      "        [0.3077, 0.1799, 0.3899]])\n",
      "k_1 tensor([[0.2629, 0.4991, 0.7752],\n",
      "        [0.2599, 0.7209, 0.3521],\n",
      "        [0.3770, 0.8617, 0.8818],\n",
      "        [0.4004, 0.7886, 0.8034],\n",
      "        [0.1051, 0.3175, 0.1360]])\n",
      "k_1_transpose tensor([[0.2629, 0.2599, 0.3770, 0.4004, 0.1051],\n",
      "        [0.4991, 0.7209, 0.8617, 0.7886, 0.3175],\n",
      "        [0.7752, 0.3521, 0.8818, 0.8034, 0.1360]])\n",
      "attension score tensor([[0.2021, 0.1511, 0.2944, 0.2634, 0.0891],\n",
      "        [0.2040, 0.1550, 0.2833, 0.2607, 0.0969],\n",
      "        [0.1956, 0.1281, 0.3303, 0.2850, 0.0610],\n",
      "        [0.1961, 0.1438, 0.3094, 0.2746, 0.0762],\n",
      "        [0.2056, 0.1813, 0.2369, 0.2284, 0.1477]])\n",
      "v1 tensor([[0.3595, 0.8356, 0.5928],\n",
      "        [0.5703, 0.6394, 0.3810],\n",
      "        [0.6570, 1.1472, 0.7748],\n",
      "        [0.5414, 0.6311, 0.3702],\n",
      "        [0.2593, 0.3150, 0.1944]])\n",
      "b1~-b4 tensor([[0.5179, 0.5142, 0.5305, 0.5242, 0.4950],\n",
      "        [0.7975, 0.7897, 0.8233, 0.8080, 0.7502],\n",
      "        [0.5203, 0.5149, 0.5380, 0.5272, 0.4878]])\n",
      "b1 tensor([0.5179, 0.7975, 0.5203])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0) # 固定随机数种子\n",
    "\n",
    "word_emb=torch.rand(5,3) # 5个词,每个词的emd宽3\n",
    "w_q=torch.rand(3,3) \n",
    "w_k=torch.rand(3,3)\n",
    "w_v=torch.rand(3,3)\n",
    "q_1=torch.matmul(word_emb,w_q)\n",
    "k_1=torch.matmul(word_emb,w_k)\n",
    "print('q_1',q_1)\n",
    "print('k_1',k_1)\n",
    "\n",
    "print('k_1_transpose',k_1.transpose(1,0))\n",
    "a=q_1@(k_1.transpose(1,0))\n",
    "a_=torch.nn.functional.softmax(a,-1)\n",
    "print('attension score', a_)\n",
    "v_1=torch.matmul(word_emb,w_v)\n",
    "print('v1', v_1)\n",
    "b=v_1.T@a_.T\n",
    "print('b1~-b4',b)\n",
    "print('b1',b[:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
